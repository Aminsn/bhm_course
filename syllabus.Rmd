---
title: "Introduction to Bayesian hierarchical modelling using R: syllabus"
author: "Andrew C Parnell"
date: "August 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Class 1: Introduction to Bayesian Statistics

Learning outcomes:
- Understand the terms prior, likelihood and posterior
- Know what a posterior probability distribution is, and why we take samples from it
- Know how to formulate of a linear regression model in a Bayesian format
- Be able to suggest appropriate prior distributions for different situations

Slides:

1. Title
1. Learning outcomes
1. Who was Bayes?
1. Why Bayes?
1. Bayes' theorem in maths and English
1. A very simple linear regression example
1. From parameters to data
1. Using prior information
1. A very basic Bayesian model with R code
1. Understanding the different parts of a Bayesian model
1. Lots of probability distributions
1. Choosing a likelihood and a prior
1. An example: linear regression
1. Simulating from the prior and the likelihood
1. Posterior computation in JAGS
1. Posterior computation in Stan
1. Stan vs JAGS
1. Calculating the posterior vs sampling from it
1. Things you can do with posterior samples
1. Summary so far: for and against Bayes
1. How to create Bayesian models: a general recipe (start with the data, fit it into a framework, LR, GLM, TS, then look at the parameters, and think of what priors are suitable)
1. Checking assumptions (e.g. residuals)
1. The danger of vague priors
1. Replication in Science and the horror of p-values
1. Example 1: 8 Schools
1. Example 2: Earnings data - https://raw.githubusercontent.com/stan-dev/example-models/master/ARM/Ch.6/earnings2.data.R A linear hierarchical model with Gaussian errors
1. Example 3: Swiss Willow Tit data - http://www.mbr-pwrc.usgs.gov/pubanalysis/roylebook/wtmatrix.csv. A logistic regression model with non-linear covariates
1. General tips: build one model for all the data, use informative priors, check your model


------

# Class 2: Linear and generalised linear models (GLMs)

Learning outcomes:

- Understand the basic formulation of a GLM in a Bayesian context
- Understand the code for a GLM in JAGS/Stan
- Be able to pick a link function for a given data set
- Know how to check model assumptions for a GLM

Slides:

1. Title
1. Learning outcomes
1. Revision: linear models
1. From LM to GLM
1. The data generating process for a standard LM
1. The data generating process for a logistic regression
1. Other types of GLM
1. Example: earnings data
1. Fitting linear regression models in JAGS
1. Fitting linear regression models in Stan
1. Checking convergence - later?
1. Link functions
1. Directed Acyclic Graphs
1. What are JAGS and Stan doing in the background?
1. Markov chain Monte Carlo
1. The Metropolis Algorithm
1. How many iterations?
1. Interpreting the output
1. Example: Swiss Willow tit data
1. Interactions
1. Checking model assumptions
1. Posterior predictive distributions
1. Other types of Binomial models
1. Poisson models
1. Offsets
1. Binomial modelling as latent data
1. Further examples of GLM-type data

------

# Practical 1: Using R, JAGS and Stan for fitting GLMs

------

# Class 3: Simple hierarchical regression models

Learning outcomes:

- Be able to build a model by thinking about the data generating process
- Be able to choose prior distributions for regression models
- Understand how to fit a hierarchical regression model in JAGS/Stan
- Be able to check the output from a regression model
- Be comfortable with plotting and manipulating the output from models


Slides:

1. Title
1. Learning outcomes
1. Thinking about models as methods for generating data
1. A more complicated DAG
1. Example: earnings data
1. Choosing a prior distribution for a regression model
1. Priors for standard deviation parameters
1. Setting up hierarchical prior distributions
1. Transformations to get a better fit
1. Causation vs explanation as goals
1. Fitting hierarchical regression models in JAGS
1. Fitting hierarchical regression models in Stan
1. Summarising the output from a regression model
1. Creating posterior predictive distributions
1. Model comparison: an introduction
1. Cross validation
1. Model information criteria
1. Bayes Factors
1. Continuous model expansion (for later)
1. Graphical model checks
1. Useful plots
1. Accessing output from JAGS
1. Accessing output from Stan
1. Summary

------

# Class 4: Hierarchical generalised linear models

Learning outcomes:

- Understand the modelling implications of moving from linear to hierarchical generalised linear models (HGLMs)
- Know some of the different versions of Hierarchical GLMs
- Be able to fit HGLMS in JAGS and Stan
- Be able to expand and summarise fitted models

Slides:

1. Title
1. Learning outcomes
1. From LMs to HGLMs
1. Type 1: Logistic-Binomial HGLMs
1. Prior distributions for Binomial HGLMs
1. logit vs probit
1. Example
1. Fitting models in JAGS
1. Fitting models in Stan
1. Type 2: Poisson HGLMs
1. Prior distributions for Poisson HGLMs
1. Offsets
1. Example
1. Fitting models in JAGS
1. Fitting models in Stan
1. Type 3: t-distributed HGLMs
1. Prior distributions on the degrees of freedom
1. Example
1. Fitting models in JAGS
1. Fitting models in Stan
1. Type 4: Ordinal data HGLMs
1. Example
1. Fitting models in JAGS
1. Fitting models in Stan
1. Summarising model output
1. Summary


------

# Practical 2: Practical: Fitting hierarchical models

------


# Class 5: Multivariate and multi-layer hierarchical models

Learning outcomes:

- Understand how to add in multiple layers to a hierarchical model
- Know how to summarise models using Analysis of Variance
- Be able to fit multivariate models in JAGS/Stan
- Be able to work with missing data in JAGS/Stan

Code:

```
???
```

Slides:

1. Title
1. Learning outcomes
1. What is a multi-layer model?
1. How many layers should I add?
1. Always put a hierarchical prior on a categorical covariate
1. Hierarchical models vs random effect models
1. Writing the same model in different ways
1. Example: multi-layer earnings data
1. Summarising multi-layer models
1. Bayesian ANOVA for multi-layer models
1. Adding and removing predictors
1. Over-dispersed models
1. Over-dispersion as a random effect
1. Example
1. Fitting models in JAGS
1. Fitting models in Stan
1. Multivariate models
1. The multivariate normal distribution
1. Priors on the mean
1. Covariance matrices and what they mean
1. Restricting the values of a covariance matrix
1. Using the Wishart distribution
1. Fitting multivariate models in JAGS
1. Fitting multivariate models in Stan
1. Example 
1. Missing data
1. Different types of missing data
1. The simple way of dealing with missing data in JAGS/Stan
1. More complex varieties of missing data
1. An example model for multivariate imputation
1. Summary



------

# Practical 3: Advanced examples of hierarchical models

------

# Class 6: Partial pooling, zero-inflation, and multinomial models

Learning outcomes:

- Be able to describe the advantages of partial pooling
- Be able to fit some basic zero inflation and hurdle models
- Be able to understand and fit some multinomial modelling examples

Code:

```
???.R
```

Slides:

1. Title
1. Learning outcomes
1. A false dichotomy: fixed vs random effects
1. The extremes of varying vs fixed parameters
1. The advantages of borrowing strength
1. Incorporating group level predictors in multi-layer models
1. Zero-inflation and hurdle models
1. The zero-inflated Poisson distribution
1. The zero-inflated negative binomial distribution
1. Fitting models with new probability distributions in JAGS
1. The ones and zeros tricks
1. Fitting models with new probability distributions in Stan
1. Example
1. Hurdle models
1. Fitting Hurdle models in Stan
1. Summarising the output of a zero-inflated model
1. The multinomial distribution
1. Example of multinomial data
1. A special case: the multinomial(1,...) distribution for classification
1. Prior distributions on probability vectors
1. Transformations vs simplex distributions
1. The Dirichlet distribution
1. The softmax transformation
1. Adding in covariates to multinomial models
1. Fitting a multinomial model in Stan
1. Example
1. Interpreting the output
1. Summary

------

# Class 7: Variable selection and non-parametrics

Learning outcomes: 

- Be able to specify complex prior distributions
- Understand the difference between shrinkage and variable selection
- Be able to fit shrinkage and selection models in JAGS/Stan
- Be able to fit some non-parametric Bayesian models

Code:

```
???.R
```

Slides:

1. Title
1. Learning outcomes
1. Really clever prior distributions
1. Ordering of parameters
1. Truncation and censoring
1. Shrinkage and variable selection
1. Lasso and Bayesian variable selection
1. The spike and slab approach
1. Ridge regression
1. The Horseshoe prior
1. Shrinkage as continuous model expansion
1. Be careful with transformations and the intercept
1. Big data vs Fat data
1. Example
1. A ridiculous word: non-parametric
1. Non-linear models 
1. Regression with basis functions
1. B-splines
1. P-splines
1. Prior distributions to induce smoothness
1. Example
1. Using the multivariate normal distribution to induce smoothness
1. Restricted prior distributions on covariance matrices
1. Gaussian processes
1. Example
1. Predictive distributions
1. Finite mixture models
1. Specifying means and variances
1. Finding the number of mixture components
1. Dirichlet Processes
1. Example
1. Summary




------

# Practical 4: Bring your own data


