---
title: 'Class 3: An introduction to Bayesian Statistics'
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{maynooth_uni_logo.jpg}
  \newline PRESS RECORD 
output:
  beamer_presentation:
    includes:
      in_header: header.tex
editor_options: 
  chunk_output_type: console
classoption: "aspectratio=169"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```

## Learning outcomes

- Understand the terms prior, likelihood and posterior
- Know what a posterior probability distribution is, and why we take samples from it
- Know how to formulate a linear regression model in a Bayesian format
- Be able to suggest appropriate prior distributions for different situations

## Who was Bayes?

*An essay towards solving a problem on the doctrine of chances* (1763)

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

\begin{center}
\includegraphics[width=4cm]{Thomas_Bayes.pdf}
\end{center}

## What is Bayesian statistics?

- Bayesian statistics is based on an interpretation of Bayes' theorem
- All quantities are divided up into _data_ (i.e. things which have been observed) and _parameters_ (i.e. things which haven't been observed)
- We use Bayes' interpretation of the theorem to get the _posterior probability distribution_, the probability of the unobserved given the observed
- Used now in almost all areas of statistical application (finance, medicine, environmetrics, gambling, etc, etc)

## Why Bayes?

The Bayesian approach has numerous advantages:

- It's easier to build complex models and to analyse the parameters you want directly
- We automatically obtain the best parameter estimates and their uncertainty from the posterior samples
- It allows us to get away from (terrible) null hypothesis testing and $p$-values

## Bayes theorem in english

Bayes' theorem can be written in words as:

$$\mbox{posterior is proportional to likelihood times prior}$$
... or ...
$$\mbox{posterior} \propto \mbox{likelihood} \times \mbox{prior}$$
  
Each of the three terms _posterior_, _likelihood_, and _prior_ are _probability distributions_ (pdfs).

In a Bayesian model, every item of interest is either data (which we will write as $x$) or parameters (which we will write as $\theta$). Often the parameters are divided up into those of interest, and other _nuisance parameters_

## Bayes theorem in maths

Bayes' equation is usually written mathematically as:
$$p(\theta|x) \propto p(x|\theta) \times p(\theta)$$
or, more fully:
$$p(\theta|x) = \frac{p(x|\theta) \times p(\theta)}{p(x)}$$

- The _posterior_ is the probability of the parameters given the data
- The _likelihood_ is the probability of observing the data given the parameters (unknowns)
- The _prior_ represents external knowledge about the parameters

## A very simple linear regression example

Suppose you had some data that looked like this:
```{r, echo=FALSE, fig.height=5}
dat = read.csv('../data/earnings.csv')
with(dat, plot(height_cm, log(earn)))
```

## What you are used to doing

\tiny
```{r}
model = lm(log(earn) ~ height_cm, data = dat)
summary(model)
```
\normalsize

## What you will now get instead

```{r, echo=FALSE, results='hide', message=FALSE}
library(R2jags)
model_code ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  intercept ~ dnorm(0,100^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
model_parameters =  c("intercept", "slope", "residual_sd")
model_run = jags(data = list(N=nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                 parameters.to.save = model_parameters,
                 model.file = textConnection(model_code))
```

\tiny

```{r}
print(model_run)
```

\normalsize 

## Using prior information

- The Bayesian model in the previous slide divided up everything into _parameters_ (the intercept, slope and residual standard deviation), and data (the height and log(earnings) values)
- The software in the background created a posterior probability distribution of the parameters given the data
- The model I fitted used no _prior information_. However, if we had done a previous experiment that suggested the intercept should be around 9 with standard deviation 0.5 we can put this in the model

## A model with prior information

```{r, echo=FALSE, results='hide', message=FALSE}
library(R2jags)
model_code ='
model {
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  intercept ~ dnorm(9,0.5^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
model_parameters =  c("intercept", "slope", "residual_sd")
model_run_2 = jags(data = list(N=nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                 parameters.to.save = model_parameters,
                 model.file = textConnection(model_code))

```

\tiny
```{r}
print(model_run_2)
```
\normalsize 

## An early example of a Bayesian model

- To create the Bayesian version of this model I used the following JAGS code:

```{r, eval = FALSE}
model_code ='
model {
  # Likelihood
  for(i in 1:N) { 
    y[i] ~ dnorm(intercept + slope*x[i], residual_sd^-2) 
  }
  # Priors
  intercept ~ dnorm(9,0.5^-2)
  slope ~ dnorm(0,100^-2)
  residual_sd ~ dunif(0,100)
}
'
```
\normalsize

## Understanding the different parts of a Bayesian model

- The likelihood is the probability of observing the data given the parameters. It represents the _data generating process_
- The prior is the probability distribution of the parameters independent from the current data you have been generated. It often requires care (and philosophy) to choose. More on this later
- The posterior is the probability distribution of the parameters given the data. It is always the target of our inference
- The two software packages we will explore (Stan and JAGS), create the posterior distribution for us

## How do I specify the prior distribution?

There are several choices when it comes to specifying prior distributions:

- _Informative_, when there is information from a previous study, or other good external source, e.g intercept $\sim N(-30,5^2)$
- _Vague_, when there is only weak information, perhaps as to the likely range of the parameter e.g. intercept $\sim N(0,100^2)$
- _Flat_, when there is no information at all about a parameter (very rare). In JAGS, write `intercept ~ dflat()`

In fact, choosing the prior and choosing the likelihood are very similar problems

## Practical differences between frequentist statistics and Bayes

- In frequentist statistics you tend to get a single best estimate of a parameter and a standard error, often assumed normally distributed, and a p-value
- In Bayesian statistics you get a large set of samples of the parameter values which match the data best. You get to choose what you do with these
- In frequentist statistics if the p-value is less than 0.05 you win. If not you cry and try a different model
- In Bayesian statistics you try to quantify the size of an effect from the posterior distribution, or find a particular posterior probability, e.g. P(slope > 0 given the data). 

## More generally choosing a likelihood and a prior

- The key to choosing a likelihood is to pick a probability distribution which matches your data, e.g. if it's a continuous measurement and is unbounded then a normal distribution. If it's count data bounded above and below then a Binomial might be appropriate
- The key to choosing a prior distribution is to choose values which you believe represent the reasonable range that the parameter can take, or come from a related study in the literature
- Again, use an _informative prior_ if possible

Note: the shape of the distribution of the response variable is usually completely unimportant when choosing the likelihood!

## Stan vs JAGS

We will be using two different software tools to calculate posterior distributions. These represent the state of the art for user-friendly, research quality Bayesian statistics.

- Stan positives: very flexible, uses sensible distribution names, everything is declared, lots of documentation support, written by people at the top of the field
- Stan negatives: cannot have discrete parameters, some odd declaration choices, slower to run code, code tends to be longer
- JAGS positives: very quick for simple models, no declarations required, a bit older than Stan so more queries answered online
- JAGS negatives: harder to get complex models running, not as fancy an algorithm as Stan, crazy way of specifying normal distributions

## Posterior computation in Stan

A Stan model looks like this:
\tiny
```{r, eval = FALSE}
stan_code = '
data {
  int N;
  vector[N] x;
  vector[N] y;
}
parameters {
  real intercept;
  real slope;
  real<lower=0> residual_sd;
} 
model {
  // Likelihood
  y ~ normal(intercept + slope * x, residual_sd);
  // Priors
  intercept ~ normal(0, 100);
  slope ~ normal(0, 100);
  residual_sd ~ uniform(0, 100);
}
'
```
\normalsize

## Running a model in Stan

To run the Stan model:
```{r, eval = FALSE}
stan_run = stan(data = list(N = nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                model_code = stan_code)
print(stan_run)
plot(stan_run)
```

## Posterior computation in JAGS

The same model in JAGS:
```{r}
jags_code = '
model{
  # Likelihood
  for(i in 1:N) {
    y[i] ~ dnorm(intercept + slope * x[i], 
                    residual_sd^-2)
  }
  # Priors
  intercept ~ dnorm(0, 100^-2)
  slope ~ dnorm(0, 100^-2)
  residual_sd ~ dunif(0, 100)
}
'
```

Note the `^-2` everywhere - JAGS uses precision rather than standard deviation in the normal distribution

## Running the JAGS mdoel

```{r, include = FALSE}
library(R2jags)
#library(rjags)
```

```{r, results = 'hide', message=FALSE}
jags_run = jags(data = list(N = nrow(dat), 
                            y = log(dat$earn),
                            x = dat$height_cm),
                parameters.to.save = c('intercept',
                                       'slope',
                                       'residual_sd'),
                model.file = textConnection(jags_code))
print(jags_run)
plot(jags_run)
```

## Calculating the posterior vs sampling from it

- There are two ways to get at a posterior:

    1. Calculate it directly using hard maths
    2. Use a simulation method

- Number 1 is impractical once you move beyond a few parameters, so number 2 is used by almost everybody
- This means that we create _samples_ from the posterior distribution. Here are three samples from the earnings example:
```{r,echo=FALSE}
posterior = model_run$BUGSoutput$sims.matrix
head(posterior, 3)
```
- We often create thousands of posterior samples to represent the posterior distribution

## Things you can do with posterior samples

- Create histograms or density plots:
- Individual summaries such as means, standard deviations, and quantiles (e.g. 95% confidence intervals)
- Joint summaries such as scatter plots or correlations
- Transformations such as logs/exponents, squares/square roots, etc

The posterior distribution will usually be stored in a matrix where each row is a sample, and each column is a different parameter. Having the posterior distribution enables you to get at exactly the quantities you are interested in

## Summary so far: for and against Bayes

For:

- A Bayesian model can be simply displayed as a likelihood and a prior. Everything is explicit
- JAGS/Stan finds the posterior distribution for us so we don't need to do any maths
- We can get exactly the quantity we are interested in, the probability distribution of our unknowns given our knowns

Against:

- It can be hard to create a prior distribution (and a likelihood)
- Not having p-values can make papers harder to publish (but this is changing)


## Checking assumptions (e.g. residuals)

- Sometimes people, because Bayesian modelling seems much richer and incorporates more information, think that their model is perfect
- In reality we need to check our assumptions. This may include:

1. Checking residuals in a linear regression model
1. Checking whether the parameter values actually match the data
1. Seeing whether a simpler or richer model might fit the data better

Some of this we will cover in later classes, but e.g. traditional residual analysis for linear regression is still important here

## The danger of vague priors

- Suppose you use the prior distribution `intercept ~ dnorm(0, 100^-2)` in JAGS because you had very little information about the intercept. Is this a reasonable thing to do? Do you honestly believe that the intercept might be as small as -200 or as big as 200?
- If you fit a model and the parameters do not match your views about the data, there must be some information you have not encoded in the prior, go back and change it!
- In more complex models, you need the prior to constrain some of the parameters so that the model will fit. These are known as _regularisation_ priors
- Use an _informative prior_ when you can

## General tips

- If you have lots of disparate data, try to build one model for all it. You'll be able to _borrow strength_ from the data (e.g. in a hierarchical model) and reduce the uncertainty in your parameter estimates

- Try your hardest to use informative priors, and always justify the values you use (especially when trying to publish). In this course we're presenting generic examples so have almost always used vague priors

- Check your model. Many of the usual requirements from traditional statistics (e.g. residual checks) are still relevant in the Bayesian world. There are some extra Bayesian tricks we can also do; discussed in later classes

## Summary

- Bayesian statistical models involve a _likelihood_ and a _prior_. These both need to be carefully chosen. From these we create a posterior distribution

- The likelihood represents the information about the data generating process; the prior information about the unknown parameters

- We usually create and analyse samples from the posterior probability distribution of the unknowns (the parameters) given the knowns (the data)

- From the posterior distribution we can create means, medians, standard deviations, credible intervals, etc, from samples we take using e.g. JAGS or Stan
