---
title: 'Class 8: Partial pooling and zero-inflation'
author: Andrew Parnell \newline \texttt{andrew.parnell@mu.ie}   \newline \vspace{1cm}
  \newline \includegraphics[width=3cm]{maynooth_uni_logo.jpg}
output:
  beamer_presentation:
    includes:
      in_header: header.tex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev = 'pdf')
#knitr::opts_knit$set(global.par = TRUE)
set.seed(123)
library(R2jags)
```
```{r, include=FALSE}
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

## Learning outcomes:

- Be able to fit some basic zero inflation and hurdle models
- Be able to understand and fit some multinomial modelling examples

\vspace{2cm}

## Zero-inflation and hurdle models

- Let's introduce some new data. This is data from an experiment on whiteflies:

```{r}
wf = read.csv('../data/whitefly.csv')
head(wf)
```

The response variable here is the count `imm` of immature whiteflies, and the explanatory variables are `block` (plant number), `week`, and  treatment `treat`.

## Look at those zeros!

```{r}
barplot(table(wf$imm), 
        main = 'Number of immature whiteflies')
```

## A first model

- These are count data so a Poisson distribution is a good start
- Let's consider a basic Poisson distribution model for $Y_{i}$, $i= 1, \ldots, N$ observations:
$$Y_i \sim Po(\lambda_i)$$
$$\log(\lambda_i) = \beta_{\mbox{trt}_i}$$
- We'll only consider the treatment effect but we could run much more complicated models with e.g. other covariates and interactions

## Fitting the model in JAGS

\small
```{r}
model_code = '
model
{
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dpois(lambda[i])
    y_pp[i] ~ dpois(lambda[i])
    log(lambda[i]) <- beta_trt[trt[i]]
  }
  # Priors
  for (j in 1:N_trt) {
    beta_trt[j] ~ dnorm(beta_mean, beta_sd^-2)
  }
  beta_mean ~ dnorm(0, 10^-2)
  beta_sd ~ dt(0, 5, 1)T(0,)
}
'
```

## Running the model

```{r, message=FALSE, results='hide'}
jags_run = 
  jags(data = list(N = nrow(wf), 
                   N_trt = length(unique(wf$trt)),
                   y = wf$imm,
                   trt = wf$trt),
       parameters.to.save = c('beta_trt', 'y_pp',
                              'beta_mean', 'beta_sd'),
       model.file = textConnection(model_code))
```

## Results

```{r, fig.height=5}
plot(jags_run)
```

Some clear treatment effects - treatment 5 in particular

## Did the model actually fit well?

```{r, fig.height=4}
y_pp = jags_run$BUGSoutput$mean$y_pp
par(mfrow=c(1,2))
hist(wf$imm, breaks = seq(0, max(wf$imm)),
     main = 'Data vs posterior predictive fit')
hist(y_pp, breaks = seq(0, max(wf$imm)), add = TRUE, col = 'gray')
plot(wf$imm, y_pp); abline(a = 0, b = 1)
```
```{r, echo = FALSE}
par(mfrow=c(1,1))
```

## What about the zeros?

- One way of broadening the distribution is through over-dispersion which we have already met:
$$\log(\lambda_i) \sim N(\beta_{\mbox{trt}_i}, \sigma^2) $$

- However this doesn't really solve the problem of excess zeros

- Instead there are a specific class of models called _zero-inflation_ models which use a specific probability distribution. The zero-inflated Poisson (ZIP) with ZI parameter $q_0$ is written as:

$$p(y|\lambda) = \left\{ \begin{array}{ll} q_0 + (1-q_0) \times Poisson(0, \lambda) & \mbox{if } y=0 \\ (1-q_0) \times Poisson(y, \lambda) & \mbox{if } y\ne 0 \end{array} \right.$$

## Fitting models with custom probability distributions

- The Zero-inflated Poisson distribution is not included in Stan or JAGS by default. We have to create it
- It's possible to create new probability distributions in Stan
- It's a little bit fiddly to do so in JAGS, we have to use some tricks
- We will use JAGS to create a mixture of Poisson distributions; A Poisson(0) distribution for the zeros, and a Poisson($\lambda$) distribution for the rest

## Fitting the ZIP in JAGS

\small
```{r}
model_code = '
model
{
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dpois(lambda[i] * z[i] + 0.0001)
    y_pp[i] ~ dpois(lambda[i] * z[i] + 0.0001)
    log(lambda[i]) <- beta_trt[trt[i]]
    z[i] ~ dbinom(q_0, 1)
  }
  # Priors
  for (j in 1:N_trt) {
    beta_trt[j] ~ dnorm(beta_mean, beta_sd^-2)
  }
  beta_mean ~ dnorm(0, 10^-2)
  beta_sd ~ dt(0, 5, 1)T(0,)
  q_0 ~ dunif(0, 1)
}
'
```

## Running the model

```{r, message=FALSE, results='hide'}
jags_run = 
  jags(data = list(N = nrow(wf), 
                   N_trt = length(unique(wf$trt)),
                   y = wf$imm,
                   trt = wf$trt),
       parameters.to.save = c('beta_trt','q_0', 'y_pp',
                              'beta_mean', 'beta_sd'),
       model.file = textConnection(model_code))
```

## Results

```{r, fig.height=5}
plot(jags_run)
```

## Did it work any better? - code

\small
```{r, fig.height = 4}
y_pp = jags_run$BUGSoutput$mean$y_pp
par(mfrow=c(1,2))
hist(wf$imm, breaks = seq(0, max(wf$imm)),
     main = 'Data vs posterior predictive fit')
hist(y_pp, breaks = seq(0, max(wf$imm)), add = TRUE, col = 'gray')
plot(wf$imm, y_pp); abline(a = 0, b = 1)
```
```{r, echo = FALSE}
par(mfrow=c(1,1))
```

## Some more notes on Zero-inflated Poisson

- This model seems to predict the number of zeros pretty well. It would also be interesting to perhaps try having a different probability of zeros ($q_0$) for different treatments
- It might be that the other covariates explain some of the zero behaviour
- We could further add in both zero-inflation and over-dispersion

## An alternative: hurdle models

- ZI models work by having a parameter (here $q_0$) which is the probability of getting a zero, and so the probability of getting a Poisson value (which could also be a zero) is 1 minus this value
- An alternative (which is slightly more complicated) is a hurdle model where $q_0$ represents the probability of the _only way_ of getting a zero. With probability (1-$q_0$) we end up with a special Poisson random variable which has to take values 1 or more
- In some ways this is richer than a ZI model since zeros can be deflated or inflated
- This is a bit fiddlier to fit in JAGS

## A hurdle-Poisson model in JAGS

\small
```{r}
model_code = '
model
{
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dpois(lambda[i])T(1,)
    log(lambda[i]) <- beta_trt[trt[i]]
  }
  for(i in 1:N_0) {
    y_0[i] ~ dbin(q_0, 1)
  }
  # Priors
  for (j in 1:N_trt) {
    beta_trt[j] ~ dnorm(beta_mean, beta_sd^-2)
  }
  beta_mean ~ dnorm(0, 10^-2)
  beta_sd ~ dt(0, 5, 1)T(0,)
  q_0 ~ dunif(0, 1)
}
'
```

## Running the model

```{r, message=FALSE, results='hide'}
jags_run = 
  jags(data = list(N = nrow(wf[wf$imm > 0,]), 
                   N_trt = length(unique(wf$trt)),
                   y = wf$imm[wf$imm > 0],
                   y_0 = as.integer(wf$imm == 0),
                   N_0 = nrow(wf),
                   trt = wf$trt[wf$imm > 0]),
       parameters.to.save = c('beta_trt', 'q_0',
                              'beta_mean', 'beta_sd'),
       model.file = textConnection(model_code))
```

## Results

\tiny
```{r, fig.height = 5}
plot(jags_run)
```

## Some final notes on ZI models

- To complete the Poisson-Hurdle fit we would need to simulate from a truncated Poisson model. This starts to get very fiddly though - see the `jags_examples` repository for worked examples
- We can extend these models further by using a better count distribution such as the negative binomial which has an extra over-dispersion parameter
- We can also add covariates into the zero-inflation component, though it is not always clear whether this is desirable

## The multinomial distribution

- Multinomial data can be thought of as multivariate discrete data
- It's usually used in two different scenarios:

    1. For classification, when you have an observation falling into a single one of $K$ possible categories
    1. For multinomial regression, where you have a set of counts which sum to a known value $N$
    
- We will just consider the multinomial regression case, whereby we have observations $y_i = \left[ y_{i1}, \ldots, y_{iK} \right]$ where the sum $\sum_{k=1}^K y_{ik} = N_i$ is fixed

- The classification version is a simplification of the regression version

## Some new data! - pollen

\tiny
```{r}
pollen = read.csv('../data/pollen.csv')
head(pollen)
```

\normalsize
These data are pollen counts of 7 varieties of pollen from modern samples with two covariates

## Some plots

- The two covariates represent the length of the growing season (GDD5) and harshness of the winter (MTCO)
- The task is to find which climate regimes each pollen variety favours
```{r, fig.height = 5, echo = FALSE}
pollen$S = rowSums(pollen[,3:ncol(pollen)])
plot(pollen$GDD5, pollen$Betula/pollen$S, xlab = 'Length of growing season', ylab = 'Estimated proportion')
```

## A multinomial model

- The multinomial distribution is often written as:
$$\left[ y_{i1}, \ldots, y_{iK} \right] \sim Mult(S_i, \left\{ p_{i1}, \ldots, p_{iK} \right\})$$
or, for short:
$$y_i \sim Mult(S_i, p_i)$$

- The key parameters here are the probability vectors $p_i$. It's these we want to use a link function on to include the covariates

- We need to be careful as each must sum to one: $\sum_{k=1}^K p_{ik} = 1$. Any link function must satisfy this constraint

## Prior distributions on probability vectors

- When $K=2$ we're back the binomial-logit we met in the first day, and we can use the logit link function
- When $K>2$ a common function to use is the _soft-max_ function:
$$p_{ik} = \frac{ \exp(z_{ik})}{ \sum_{j=1}^K \exp(z_{ij})}$$
- This is a generalisation of the logit function
- The next layer of our model sets, e.g.:
$$z_{ik} = \beta_0 + \beta_1 \mbox{GDD5}_i + \gamma_2 \mbox{MTCO}_i + \ldots$$

## JAGS code

\small
```{r}
model_code = '
model
{
  # Likelihood
  for (i in 1:N) { # Observaton loops
    y[i,] ~ dmulti(p[i,], S[i])
    for(j in 1:M) { # Category loop
      exp_z[i,j] <- exp(z[i,j])
      p[i,j] <- exp_z[i,j]/sum(exp_z[i,])
      z[i,j] <- beta[j,]%*%x[i,]
    }
  }
  # Prior
  for(j in 1:M) {
    for(k in 1:K) {
      beta[j,k] ~ dnorm(0, 0.1^-2)
    }
  }
}
'
```

## Let's fit it (first 500 obs only)

```{r, results = 'hide', messages = FALSE}
model_data = list(N = nrow(pollen[1:500,]), 
                  y = pollen[1:500,3:9],
                  x = cbind(1, scale(cbind(pollen[1:500,1:2],
                                           pollen[1:500,1:2]^2))),
                  S = pollen[1:500,10],
                  K = 5, # Number of covars
                  M = 7) # Number of categories
# Run the model
model_run = jags(data = model_data,
                 parameters.to.save = c("p"),
                 model.file = textConnection(model_code))
```

## Results 1

```{r}
plot(model_run)
```

## Results 2

```{r, fig.height = 6, echo = FALSE}
p3_model = model_run$BUGSoutput$mean$p[,3]
p3_data = pollen$Betula/pollen$S
plot(p3_data[1:500], p3_model,
     xlab = 'True proportion of Betula',
     ylab = 'Estimated proportion of Betula')
abline(a=0, b=1)
```

## Notes about this model

- This model is not going to fit very well, since it is unlikely that a linear relationship between the covariates and the pollen counts will match the data
- It might be better to use e.g. a spline model (covered in the next class)
- Similarly we might need some complex interactions between the covariates as they are strongly linked
- We have constrained the parameters here so that the slopes and intercepts borrow strength across species. Does this make sense? What else could we do?

## Some final notes about multinomial models

- These models can be a pain to deal with as there are tricky constraints on the $\beta$ parameters to make them all sum to 1. Instead it's often easier to just put a tight prior distribution on them, e.g. $\beta \sim N(0, 0.1)$ 
- The `softmax` function is one choice but there are lots of others (logistic ratios, the Dirichlet distribution, ...)
- Whilst the classification version of this model just has binary $y_i$ (with just a single 1 in it, i.e. $S_i = 1$) most packages (including JAGS and Stan) have a special distribution (e.g. `dcat` in JAGS) for this situation

## Summary

- We have fitted some zero inflated and hurdle Poisson models in JAGS
- We have seen a simple multinomial logistic regression
